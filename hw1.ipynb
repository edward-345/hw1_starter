{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_ChUFBpzd0a"
   },
   "source": [
    "This is a notebook for HW1. Be careful that variable values persist, so you may end up getting strange errors that are only solved when you restart the runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "0eCdzLUUzHVC"
   },
   "outputs": [],
   "source": [
    "# These are imports and you do not need to modify these.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import random\n",
    "import math\n",
    "import urllib.request\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(3142025)\n",
    "random.seed(3142025)\n",
    "\n",
    "def load_data():\n",
    "    # Load the data\n",
    "    with urllib.request.urlopen(\"https://www.cs.toronto.edu/~cmaddis/courses/sta314_f25/data/clean_fake.txt\") as f:\n",
    "        fake = [l.decode(\"utf-8\").strip() for l in f]\n",
    "    with urllib.request.urlopen(\"https://www.cs.toronto.edu/~cmaddis/courses/sta314_f25/data/clean_real.txt\") as f:\n",
    "        real = [l.decode(\"utf-8\").strip() for l in f]\n",
    "\n",
    "    # Each element is a string, corresponding to a headline\n",
    "    data = np.array(real + fake)\n",
    "    labels = np.array([0]*len(real) + [1]*len(fake))\n",
    "    return data, labels\n",
    "\n",
    "def main():\n",
    "    data, labels = load_data()\n",
    "    train_X, train_Y, val_X, val_Y, test_X, test_Y = process_data(data, labels)\n",
    "\n",
    "    best_model, best_k = select_knn_model(train_X, val_X, train_Y, val_Y)\n",
    "    test_accuracy = best_model.score(test_X, test_Y)\n",
    "    print(\"Selected K: {}\".format(best_k))\n",
    "    print(\"Test Acc: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IdwIY8YfzqNo"
   },
   "source": [
    "You need to fill in the following two functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "XfuC9nQPzQy1"
   },
   "outputs": [],
   "source": [
    "def process_data(data, labels):\n",
    "    \"\"\"\n",
    "    Preprocess a dataset of strings into vector representations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: numpy array\n",
    "        An array of N strings.\n",
    "    labels: numpy array\n",
    "        An array of N integer labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_X: numpy array\n",
    "        Array with shape (N, D) of N inputs.\n",
    "    train_Y:\n",
    "        Array with shape (N,) of N labels.\n",
    "    val_X:\n",
    "        Array with shape (M, D) of M inputs.\n",
    "    val_Y:\n",
    "        Array with shape (M,) of M labels.\n",
    "    test_X:\n",
    "        Array with shape (M, D) of M inputs.\n",
    "    test_Y:\n",
    "        Array with shape (M,) of M labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the dataset of string into train, validation, and test \n",
    "    # Use a 70/15/15 split\n",
    "    # train_test_split shuffles the data before splitting it \n",
    "    # Stratify keeps the proportion of labels the same in each split\n",
    "\n",
    "    train_X, temp_X, train_Y, temp_Y = train_test_split(data, labels,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t test_size = 0.7,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t random_state = 67,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t stratify = labels)\n",
    "\t# train_X is 70% of \"data\" array, train_Y is 70% of \"labels\" array\n",
    "\n",
    "    val_X, test_X, val_Y, test_Y = train_test_split(temp_X,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t temp_Y,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t test_size = 0.5,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t random_state=67,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t stratify = temp_Y)\n",
    "\n",
    "    # Preprocess each dataset of strings into a dataset of feature vectors\n",
    "    # using the CountVectorizer function. \n",
    "    # Note, fit the Vectorizer using the training set only, and then\n",
    "    # transform the validation and test sets.\n",
    "\n",
    "    # Initializing CountVectorizer()\n",
    "    vectorizer = CountVectorizer(\n",
    "\t\tinput = 'filename',\n",
    "\t\tdecode_error = 'strict',\n",
    "\t\ttoken_pattern = r\"(?u)\\b\\w+\\b\",\n",
    "\t\tngram_range = (1, 2),\n",
    "\t\tanalyser = 'word'\n",
    "\t\t)\n",
    "\n",
    "    # Fitting and transforming train X, Y\n",
    "    train_X = vectorizer.fit_transform(train_X)\n",
    "    train_Y = vectorizer.fit_transform(train_Y)\n",
    "\n",
    "\t# Transforming test and val X, Y\n",
    "    val_X = vectorizer.transform(val_X)\n",
    "    val_Y = vectorizer.transform(val_Y)\n",
    "    \n",
    "    test_X = vectorizer.transform(test_X)\n",
    "    test_Y = vectorizer.transform(test_Y)\n",
    "\n",
    "    # Return the training, validation, and test set inputs and labels\n",
    "\n",
    "    return(train_X, train_Y, val_X, val_Y, test_X, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Z2E0tFcZzbu6"
   },
   "outputs": [],
   "source": [
    "def select_knn_model(train_X, val_X, train_Y, val_Y):\n",
    "    \"\"\"\n",
    "    Test k in {1, ..., 20} and return the a k-NN model\n",
    "    fitted to the training set with the best validation loss.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_X: numpy array\n",
    "        Array with shape (N, D) of N inputs.\n",
    "    train_Y: numpy array\n",
    "        Array with shape (M, D) of M inputs.\n",
    "    train_Y: numpy array\n",
    "        Array with shape (N,) of N labels.\n",
    "    val_Y: numpy array\n",
    "        Array with shape (M,) of M labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_model : KNeighborsClassifier\n",
    "        The best k-NN classifier fit on the training data \n",
    "    and selected according to validation loss.\n",
    "    best_k : int\n",
    "        The best k value according to validation loss.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BY13RYNLztWg"
   },
   "source": [
    "Run the next cell to get the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "xg8Fb725zv_Q"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CountVectorizer.__init__() got an unexpected keyword argument 'analyser'. Did you mean 'analyzer'?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m main()\n",
      "Cell \u001b[0;32mIn[8], line 31\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m     30\u001b[0m     data, labels \u001b[38;5;241m=\u001b[39m load_data()\n\u001b[0;32m---> 31\u001b[0m     train_X, train_Y, val_X, val_Y, test_X, test_Y \u001b[38;5;241m=\u001b[39m process_data(data, labels)\n\u001b[1;32m     33\u001b[0m     best_model, best_k \u001b[38;5;241m=\u001b[39m select_knn_model(train_X, val_X, train_Y, val_Y)\n\u001b[1;32m     34\u001b[0m     test_accuracy \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mscore(test_X, test_Y)\n",
      "Cell \u001b[0;32mIn[12], line 51\u001b[0m, in \u001b[0;36mprocess_data\u001b[0;34m(data, labels)\u001b[0m\n\u001b[1;32m     39\u001b[0m     val_X, test_X, val_Y, test_Y \u001b[38;5;241m=\u001b[39m train_test_split(temp_X,\n\u001b[1;32m     40\u001b[0m \t\t\t\t\t\t\t\t\t\t\t\t temp_Y,\n\u001b[1;32m     41\u001b[0m \t\t\t\t\t\t\t\t\t\t\t\t test_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m,\n\u001b[1;32m     42\u001b[0m \t\t\t\t\t\t\t\t\t\t\t\t random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m67\u001b[39m,\n\u001b[1;32m     43\u001b[0m \t\t\t\t\t\t\t\t\t\t\t\t stratify \u001b[38;5;241m=\u001b[39m temp_Y)\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# Preprocess each dataset of strings into a dataset of feature vectors\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# using the CountVectorizer function. \u001b[39;00m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# Note, fit the Vectorizer using the training set only, and then\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# transform the validation and test sets.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# Initializing CountVectorizer()\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(\n\u001b[1;32m     52\u001b[0m \t\t\u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     53\u001b[0m \t\tdecode_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     54\u001b[0m \t\ttoken_pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(?u)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     55\u001b[0m \t\tngram_range \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     56\u001b[0m \t\tanalyser \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     57\u001b[0m \t\t)\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# Fitting and transforming train X, Y\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     train_X \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(train_X)\n",
      "\u001b[0;31mTypeError\u001b[0m: CountVectorizer.__init__() got an unexpected keyword argument 'analyser'. Did you mean 'analyzer'?"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
